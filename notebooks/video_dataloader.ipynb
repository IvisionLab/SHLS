{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c34c125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import data #import Dataset, IterableDataset#, Dataloader\n",
    "\n",
    "class MyMapDataset(data.Dataset):\n",
    "    def __init__(self, data_):\n",
    "        self.data_ = data_\n",
    "    def __len__(self):\n",
    "        return len(self.data_)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_[idx]\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a08e651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n",
      "tensor([4, 5, 6, 7])\n",
      "tensor([ 8,  9, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "data_ = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "map_dataset = MyMapDataset(data_)\n",
    "loader = data.DataLoader(map_dataset, batch_size=4)\n",
    "for batch in loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4fc33ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n",
      "tensor([4, 5, 6, 7])\n",
      "tensor([ 8,  9, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MyIterableDataset_simple(data.IterableDataset):\n",
    "    def __init__(self, data_):\n",
    "        self.data_ = data_\n",
    "    def __iter__(self):\n",
    "        return iter(self.data_)\n",
    "\n",
    "iterable_dataset = MyIterableDataset_simple(data_)\n",
    "loader = data.DataLoader(iterable_dataset, batch_size=4)\n",
    "for batch in loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6f361019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picked:  [0, 1, 2, 3]\n",
      "num_pick:  [2, 1, 1]\n",
      "[12, 31, 40]\n",
      "[13, 32, 41]\n",
      "[14, 33, 42]\n",
      "[15, 34, 43]\n",
      "[16, 35, -1]\n",
      "[17, 36, -1]\n",
      "[27, 37, -1]\n",
      "[28, 38, -1]\n",
      "[29, 39, -1]\n",
      "[30, -1, -1]\n",
      "-----------\n",
      "picked:  [0, 1, 2, 3]\n",
      "num_pick:  [2, 1, 1]\n",
      "[12, 31, 40]\n",
      "[13, 32, 41]\n",
      "[14, 33, 42]\n",
      "[15, 34, 43]\n",
      "[16, 35, -1]\n",
      "[17, 36, -1]\n",
      "[27, 37, -1]\n",
      "[28, 38, -1]\n",
      "[29, 39, -1]\n",
      "[30, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "from itertools import cycle, islice, chain, zip_longest\n",
    "import random\n",
    "\n",
    "class MyIterableDataset(IterableDataset):\n",
    "       \n",
    "    def __init__(self, data_list, batch_size):\n",
    "        self.data_list = data_list\n",
    "        self.batch_size = batch_size\n",
    "        self.reset()\n",
    "        \n",
    "    \n",
    "    def reset(self):\n",
    "        self.picked = list(range(len(self.data_list)))\n",
    "        self.num_pick = [1 for _ in range(self.batch_size)]\n",
    "        \n",
    "        cc = len(self.data_list) - self.batch_size\n",
    "        while cc > 0:\n",
    "            for i in range(self.batch_size):          \n",
    "                self.num_pick[i] += 1\n",
    "                cc -= 1\n",
    "                if cc <=0:\n",
    "                    break\n",
    "        \n",
    "        print('picked: ', self.picked)\n",
    "        print('num_pick: ', self.num_pick)\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def shuffled_data_list(self):\n",
    "        if self.picked:\n",
    "            r_list = self.data_list[self.picked[0]:self.picked[0]+self.num_pick[0]]\n",
    "            for _ in range(self.num_pick[0]):\n",
    "                self.picked.pop(0)\n",
    "            self.num_pick.pop(0)\n",
    "            return r_list\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "\n",
    "    def process_data(self, data):\n",
    "        for x in data:\n",
    "            yield x\n",
    "\n",
    "    def get_stream(self, data_list):\n",
    "        return chain.from_iterable(map(self.process_data, data_list))\n",
    "        \n",
    "\n",
    "    def get_streams(self):\n",
    "        a_list = []\n",
    "        for l in range(self.batch_size):\n",
    "            a_list.append(self.get_stream(self.shuffled_data_list))\n",
    "        return zip_longest(*a_list,fillvalue=-1)\n",
    "        \n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.get_streams()\n",
    "    \n",
    "data_list = [\n",
    " [12, 13, 14, 15, 16, 17],\n",
    " [27, 28, 29,30],\n",
    " [31, 32, 33, 34, 35, 36, 37, 38, 39],\n",
    " [40, 41, 42, 43],]    \n",
    "\n",
    "\n",
    "iterable_dataset = MyIterableDataset(data_list, batch_size=3)\n",
    "loader = data.DataLoader(iterable_dataset, batch_size=None)\n",
    "\n",
    "for batch in islice(loader, None):\n",
    "    print(batch)\n",
    "print('-----------')\n",
    "iterable_dataset.reset()\n",
    "for batch in islice(loader, None):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3ec66b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 40, 40, 31]\n",
      "[32, 41, 41, 32]\n",
      "[33, 42, 42, 33]\n",
      "[34, 43, 43, 34]\n",
      "[35, 31, 12, 35]\n",
      "[36, 32, 13, 36]\n",
      "[37, 33, 14, 37]\n",
      "[38, 34, 15, 38]\n",
      "[39, 35, 16, 39]\n",
      "[12, 36, 17, 27]\n",
      "[13, 37, 31, 28]\n",
      "[14, 38, 32, 29]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "class MyIterableDataset_s(IterableDataset):\n",
    "\n",
    "    def __init__(self, data_list, batch_size):\n",
    "        self.data_list = data_list\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    @property\n",
    "    def shuffled_data_list(self):\n",
    "        return random.sample(self.data_list, len(self.data_list))\n",
    "\n",
    "    def process_data(self, data):\n",
    "        for x in data:\n",
    "            worker = torch.utils.data.get_worker_info()\n",
    "            worker_id = id(self) if worker is not None else -1\n",
    "            start = time.time()\n",
    "            time.sleep(0.1)\n",
    "            end = time.time()\n",
    "            yield x#, worker_id, start, end\n",
    "\n",
    "    def get_stream(self, data_list):\n",
    "        return chain.from_iterable(map(self.process_data, cycle(data_list)))\n",
    "\n",
    "    def get_streams(self):\n",
    "        return zip(*[self.get_stream(self.shuffled_data_list)\n",
    "                            for _ in range(self.batch_size)])\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.get_streams()\n",
    "\n",
    "    @classmethod\n",
    "    def split_datasets(cls, data_list, batch_size, max_workers):\n",
    "\n",
    "        for n in range(max_workers, 0, -1):\n",
    "            if batch_size % n == 0:\n",
    "                num_workers = n\n",
    "                break\n",
    "\n",
    "        split_size = batch_size // num_workers\n",
    "        return [cls(data_list, batch_size=split_size)\n",
    "                        for _ in range(num_workers)]\n",
    "\n",
    "class MultiStreamDataLoader:\n",
    "\n",
    "    def __init__(self, datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def get_stream_loaders(self):\n",
    "        return zip(*[data.DataLoader(dataset, num_workers=1, batch_size=None)\n",
    "                                                 for dataset in datasets])\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch_parts in self.get_stream_loaders():\n",
    "            yield list(chain(*batch_parts))\n",
    "\n",
    "            \n",
    "data_list = [\n",
    " [12, 13, 14, 15, 16, 17],\n",
    " [27, 28, 29],\n",
    " [31, 32, 33, 34, 35, 36, 37, 38, 39],\n",
    " [40, 41, 42, 43],]    \n",
    "            \n",
    "datasets = MyIterableDataset_s.split_datasets(data_list, batch_size=4, max_workers=1)\n",
    "loader = MultiStreamDataLoader(datasets)\n",
    "for batch in islice(loader, 12):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "5e4c20f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_workers:  2\n",
      "data_lists:  2\n",
      "split_size:  2\n",
      "[0] 2 lists: [list([31, 32, 33, 34, 35, 36, 37, 38, 39, 40])\n",
      " list([12, 13, 14, 15, 16, 17])]\n",
      "[1] 2 lists: [list([40, 41, 42, 43, 44]) list([27, 28, 29])]\n",
      "picked:  [0, 1]\n",
      "num_pick:  [1, 1]\n",
      "picked:  [0, 1]\n",
      "num_pick:  [1, 1]\n",
      "\n",
      "\n",
      "[[31, 67], [12, 67], [40, 76], [27, 76]]\n",
      "[[32, 67], [13, 67], [41, 76], [28, 76]]\n",
      "[[33, 67], [14, 67], [42, 76], [29, 76]]\n",
      "[[34, 67], [15, 67], [43, 76], [-1, -1]]\n",
      "[[35, 67], [16, 67], [44, 76], [-1, -1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def getSum(n):    \n",
    "    sum = 0\n",
    "    for digit in str(n): \n",
    "      sum += int(digit)      \n",
    "    return sum\n",
    "\n",
    "class MyIterableDataset_s2(IterableDataset):\n",
    "\n",
    "    def __init__(self, data_list, batch_size):\n",
    "        self.data_list = data_list\n",
    "        self.batch_size = batch_size\n",
    "        self.reset()\n",
    "        \n",
    "    \n",
    "    def reset(self):\n",
    "        self.picked = list(range(len(self.data_list)))\n",
    "        self.num_pick = [1 for _ in range(self.batch_size)]\n",
    "        \n",
    "        cc = len(self.data_list) - self.batch_size\n",
    "        while cc > 0:\n",
    "            for i in range(self.batch_size):          \n",
    "                self.num_pick[i] += 1\n",
    "                cc -= 1\n",
    "                if cc <=0:\n",
    "                    break\n",
    "        \n",
    "        print('picked: ', self.picked)\n",
    "        print('num_pick: ', self.num_pick)\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def shuffled_data_list(self):\n",
    "        if self.picked:\n",
    "            r_list = self.data_list[self.picked[0]:self.picked[0]+self.num_pick[0]]\n",
    "            for _ in range(self.num_pick[0]):\n",
    "                self.picked.pop(0)\n",
    "            self.num_pick.pop(0)\n",
    "            return r_list\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def process_data(self, data):\n",
    "        for x in data:\n",
    "            worker = torch.utils.data.get_worker_info()\n",
    "            worker_id = id(self) if worker is not None else -1\n",
    "            start = time.time()\n",
    "            time.sleep(0.1)\n",
    "            end = time.time()\n",
    "            yield x, getSum(worker_id)#, start, end\n",
    "\n",
    "    def get_stream(self, data_list):\n",
    "        return chain.from_iterable(map(self.process_data, data_list))\n",
    "\n",
    "    def get_streams(self):\n",
    "        a_list = []\n",
    "        for l in range(self.batch_size):\n",
    "            a_list.append(self.get_stream(self.shuffled_data_list))\n",
    "        return zip_longest(*a_list,fillvalue=[-1,-1])\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.get_streams()\n",
    "\n",
    "    @classmethod\n",
    "    def split_datasets(cls, data_list, batch_size, max_workers):\n",
    "\n",
    "        for n in range(max_workers, 0, -1):\n",
    "            if batch_size % n == 0:\n",
    "                num_workers = n\n",
    "                break\n",
    "        print('num_workers: ', num_workers)\n",
    "        data_list.sort(key=len, reverse=True)\n",
    "        data_lists = np.array_split(data_list, num_workers)\n",
    "        print('data_lists: ', len(data_lists))\n",
    "        split_size = batch_size // num_workers\n",
    "        print('split_size: ', split_size)\n",
    "        \n",
    "        for x,i in enumerate(data_lists):\n",
    "            print('[{}] {} lists: {}'.format(x,len(i),i))\n",
    "        return [cls(data_lists[i], batch_size=split_size)\n",
    "                        for i in range(num_workers)]\n",
    "    \n",
    "    def pad_lists(self, data_lists):\n",
    "        max_size = 0\n",
    "        \n",
    "        \n",
    "\n",
    "class MultiStreamDataLoader:\n",
    "\n",
    "    def __init__(self, datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def get_stream_loaders(self):\n",
    "        return zip(*[data.DataLoader(dataset, num_workers=1, batch_size=None)\n",
    "                                                 for dataset in datasets])\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch_parts in self.get_stream_loaders():\n",
    "            yield list(chain(*batch_parts))\n",
    "\n",
    "            \n",
    "data_list = [\n",
    " [12, 13, 14, 15, 16, 17],\n",
    " [27, 28, 29],\n",
    " [31, 32, 33, 34, 35, 36, 37, 38, 39, 40],\n",
    " [40, 41, 42, 43, 44],]     \n",
    "           \n",
    "datasets = MyIterableDataset_s2.split_datasets(data_list, batch_size=4, max_workers=2)\n",
    "loader = MultiStreamDataLoader(datasets)\n",
    "print('\\n') \n",
    "for batch in islice(loader, None):\n",
    "    print(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
